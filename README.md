# Goshen AI - AI Inference Platform

An AI inference platform (similar to fal.ai or Replicate) that lets users access image and video generation models through a simple API. Currently in MVP stage, designed to run locally on a single GPU (scalable to multiple GPUs later).

## ğŸ§  Project Goal

Create a production-ready, developer-friendly service for hosting and running inference on multiple AI models. Developers can generate media through API keys, while authenticated users (admins) can manage models, jobs, and API keys via a dashboard.

## âš™ï¸ Tech Stack

* **Frontend & Dashboard:** Next.js 15 + shadcn/ui + React Server Components
* **Backend:** Elysia (Bun) â€” serves internal compute and /v1 API routes
* **Auth:** WorkOS (via @workos-inc/authkit-nextjs) with global middleware protection
* **Database:** PostgreSQL (Neon â†’ PlanetScale when scaling) with Drizzle ORM
* **Cache / Queue:** Redis (Railway) for background jobs, queueing, and rate-limiting
* **Storage:** S3/R2 for artifacts (images/videos)
* **Infra:** RunPod.io for GPU inference nodes (local for MVP, distributed later)
* **Schema / Contracts:** Shared package with zod + Drizzle types

---

## âœ… What's Been Implemented

### Infrastructure & Setup
* âœ… Monorepo structure with pnpm workspace (apps, packages, services)
* âœ… TypeScript configuration across all packages
* âœ… Shared UI component library with shadcn/ui
* âœ… ESLint configuration packages

### Authentication & Authorization
* âœ… WorkOS authentication integration with AuthKit
* âœ… Global middleware protection for authenticated routes
* âœ… Auth webhook endpoint for user lifecycle events
* âœ… User database schema with auth events tracking

### Database & Schema
* âœ… Drizzle ORM setup with PostgreSQL
* âœ… Database migrations (4 migrations completed)
* âœ… Schema definitions:
  - Users table
  - Auth events table
  - API keys table (with hashing and last4 display)
  - Jobs table (with status tracking, timing, GPU metrics)

### API & Backend
* âœ… Elysia backend with Next.js catch-all API route
* âœ… CORS configuration for API access
* âœ… Swagger/OpenAPI documentation (`/api/docs`)
* âœ… API key generation and management endpoints:
  - `POST /api/admin/keys` - Generate new API key
  - `GET /api/admin/keys` - List user's API keys
  - `DELETE /api/admin/keys/:id` - Delete API key (stubbed)
* âœ… API key hashing with SHA-256 and secure storage
* âœ… Generate endpoint: `POST /api/v1/generate`

### Job Queue & Worker
* âœ… BullMQ setup with Redis connection
* âœ… Generate queue configuration with retry logic
* âœ… Worker process with concurrency control
* âœ… Job queue integration in generate endpoint
* âœ… Database schema for job tracking (status, timing, results)
* âœ… Graceful shutdown handling

### Dashboard & Frontend
* âœ… Next.js 15 App Router structure
* âœ… Dashboard home page with model listing
* âœ… Dynamic model detail pages (`/dashboard/model/[id]`)
* âœ… Profile page
* âœ… Code display components for API examples
* âœ… Generate button component
* âœ… Header and navbar components
* âœ… Toast notifications (Sonner)

---

## âŒ What's Left To Do

### Core Inference & GPU
* âŒ RunPod.io integration for GPU nodes
* âŒ Actual model inference logic in worker process
* âŒ Model container/Docker setup for RunPod
* âŒ GPU health check and availability tracking
* âŒ Multi-GPU load balancing logic

### Storage & Assets
* âŒ S3/R2 storage integration for generated artifacts
* âŒ Pre-signed URL generation for secure asset access
* âŒ Asset cleanup/retention policies
* âŒ Image/video optimization pipeline

### API Features
* âŒ API key authentication middleware implementation
* âŒ Rate limiting per API key
* âŒ Usage tracking and metering
* âŒ Job status polling endpoint (`GET /api/v1/jobs/:id`)
* âŒ Webhook callbacks for job completion
* âŒ Streaming/real-time progress updates

### Model Management
* âŒ Model CRUD endpoints (create, read, update, delete)
* âŒ Model version management
* âŒ Model configuration schema (inputs, outputs, parameters)
* âŒ Model deployment and health status tracking
* âŒ Model catalog with metadata (pricing, speed, examples)

### Dashboard Features
* âŒ Job history and monitoring page
* âŒ Usage analytics and metrics
* âŒ API key management UI
* âŒ Model management interface
* âŒ Real-time job status updates
* âŒ Cost/usage dashboard

### Billing & Monetization
* âŒ Usage metering per job
* âŒ Credit/balance system
* âŒ Pricing tiers per model
* âŒ Billing integration (Stripe/similar)
* âŒ Invoice generation

### Production & Operations
* âŒ Environment configuration management
* âŒ Error tracking (Sentry/similar)
* âŒ Logging and monitoring
* âŒ Production deployment configs (Docker, CI/CD)
* âŒ Database connection pooling optimization
* âŒ Caching strategy for frequently accessed data
* âŒ API rate limiting with Redis

### Documentation
* âŒ API usage documentation and examples
* âŒ Model-specific integration guides
* âŒ Quickstart guide for developers
* âŒ Deployment guide
* âŒ Architecture diagrams

### Testing & Quality
* âŒ Unit tests for API endpoints
* âŒ Integration tests for job queue
* âŒ End-to-end tests for dashboard
* âŒ Load testing for API throughput

---

## ğŸš€ Quick Start

### Prerequisites
```bash
pnpm install
```

### Environment Variables
Set up `.env` files in:
- `apps/web/.env.local` - WorkOS keys, database URL
- `apps/worker/.env` - Redis connection, database URL

### Running Locally
```bash
# Start web app
pnpm --filter web dev

# Start worker
pnpm --filter worker-server dev
```

### Adding UI Components
```bash
pnpm dlx shadcn@latest add button -c apps/web
```

---

## ğŸ“ Project Structure

```
goshen-ai/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ web/              # Next.js dashboard & API proxy
â”‚   â””â”€â”€ worker/           # BullMQ worker for job processing
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ db/               # Drizzle schema & migrations
â”‚   â”œâ”€â”€ ui/               # Shared shadcn/ui components
â”‚   â”œâ”€â”€ eslint-config/    # Shared ESLint configs
â”‚   â””â”€â”€ typescript-config/ # Shared TS configs
â””â”€â”€ services/
    â””â”€â”€ gpu/              # GPU inference services (WIP)
```
